# ğŸ¤– AI-Powered Coding Assignment Evaluator

> **Automated. Fair. Explainable. Scalable.**  
> An intelligent system that evaluates code submissions beyond just correctness â€” assessing quality, efficiency, readability, and providing constructive, human-readable feedback at scale.

---

## ğŸ“Œ Table of Contents

- [Problem Statement](#-problem-statement)
- [Solution Overview](#-solution-overview)
- [Key Features](#-key-features)
- [System Architecture](#-system-architecture)
- [Evaluation Dimensions](#-evaluation-dimensions)
- [How It Works](#-how-it-works)
- [Tech Stack](#-tech-stack)
- [Getting Started](#-getting-started)
- [API Reference](#-api-reference)
- [Feedback Report Format](#-feedback-report-format)
- [Use Cases](#-use-cases)
- [Roadmap](#-roadmap)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸš¨ Problem Statement

The current state of coding assessments â€” in classrooms, bootcamps, and technical hiring â€” is fundamentally broken in five key ways:

| Problem | Impact |
|---|---|
| â³ **Delayed Feedback** | Manual evaluation slows learning and iteration cycles |
| ğŸ“Š **Surface-Level Scoring** | Most systems only check test case pass/fail, ignoring code quality and design |
| ğŸ” **Lack of Explainability** | Candidates don't understand *why* their solution was weak or strong |
| ğŸ“ˆ **Scalability Issues** | Instructors and recruiters can't deeply review hundreds of submissions |
| âš–ï¸ **Unfair Evaluation** | Different evaluators apply inconsistent, subjective standards |

---

## ğŸ’¡ Solution Overview

The **AI-Powered Coding Assignment Evaluator** is an intelligent evaluation pipeline that automatically analyzes code submissions along multiple quality dimensions â€” not just correctness â€” and generates clear, constructive, and consistent feedback for every learner, every time.

```
Student Submits Code
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Submission Layer   â”‚  â† Accepts code via API, web UI, or LMS integration
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Static Analysis     â”‚  â† Syntax, complexity, style checks (AST-based)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dynamic Execution   â”‚  â† Test case runner + edge case detection (sandboxed)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Evaluation Layer â”‚  â† LLM-powered deep analysis (quality, design, patterns)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Feedback Generator  â”‚  â† Structured, human-readable, constructive report
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   Instructor Dashboard / Student Portal / LMS
```

---

## âœ¨ Key Features

### ğŸ¯ Multi-Dimensional Evaluation
Goes far beyond pass/fail â€” evaluates code quality, algorithmic efficiency, readability, design patterns, and edge case handling.

### ğŸ§  AI-Powered Code Understanding
Uses large language models to understand *intent*, not just execution. Identifies anti-patterns, over-engineering, and missed opportunities â€” just like an experienced code reviewer would.

### ğŸ“ Constructive Feedback Generation
Every submission receives a detailed, empathetic report explaining what was done well, what needs improvement, and *how* to improve it â€” with code snippets and suggestions.

### âš–ï¸ Consistent & Bias-Free Scoring
Rubric-based AI evaluation ensures every student or candidate is measured against the same objective standard â€” eliminating evaluator fatigue and subjective bias.

### âš¡ Real-Time Results
Feedback is returned in seconds, not hours â€” enabling tight learning loops and rapid skill iteration.

### ğŸ“Š Instructor Analytics Dashboard
Aggregate insights across all submissions: class-wide weak areas, plagiarism detection, grade distributions, and individual progress tracking.

### ğŸ”Œ LMS & API Integration
REST API-first design integrates with Canvas, Moodle, Google Classroom, Gradescope, and custom hiring platforms.

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CLIENT LAYER                            â”‚
â”‚   Web UI  â”‚  Mobile App  â”‚  LMS Plugin  â”‚  REST API / CLI     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API GATEWAY                               â”‚
â”‚          Auth  â”‚  Rate Limiting  â”‚  Request Routing            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Submission  â”‚  â”‚  Execution      â”‚  â”‚  AI Evaluation      â”‚
â”‚  Service     â”‚  â”‚  Sandbox        â”‚  â”‚  Service            â”‚
â”‚              â”‚  â”‚  (Docker/WASM)  â”‚  â”‚  (LLM + Rubrics)    â”‚
â”‚ â€¢ File parse â”‚  â”‚ â€¢ Test runner   â”‚  â”‚ â€¢ Code quality      â”‚
â”‚ â€¢ Language   â”‚  â”‚ â€¢ Memory limits â”‚  â”‚ â€¢ Pattern detect    â”‚
â”‚   detection  â”‚  â”‚ â€¢ Time limits   â”‚  â”‚ â€¢ Feedback gen      â”‚
â”‚ â€¢ Queue mgmt â”‚  â”‚ â€¢ I/O capture   â”‚  â”‚ â€¢ Score aggregation â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA LAYER                               â”‚
â”‚     PostgreSQL  â”‚  Redis Cache  â”‚  S3 (submissions/reports)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  REPORTING & ANALYTICS                         â”‚
â”‚    Instructor Dashboard  â”‚  Student Portal  â”‚  Export (PDF/CSV) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Evaluation Dimensions

Each submission is scored across **6 core dimensions**, each weighted within a configurable rubric:

### 1. âœ… Correctness (0â€“25 pts)
- Unit test pass rate
- Expected vs. actual output comparison
- Return value and type correctness
- Error handling (does it crash gracefully?)

### 2. âš™ï¸ Efficiency & Algorithmic Performance (0â€“20 pts)
- **Time Complexity**: Is the algorithm optimal? (O(nÂ²) when O(n log n) is possible?)
- **Space Complexity**: Unnecessary memory usage? Memory leaks?
- **Runtime benchmarks**: Execution time against large inputs
- Comparison against baseline optimal solution

### 3. ğŸ“– Code Readability (0â€“15 pts)
- Naming conventions (variables, functions, classes)
- Code formatting and indentation consistency
- Comment quality (are complex sections explained?)
- Function length and single-responsibility adherence
- Avoidance of "magic numbers" and ambiguous identifiers

### 4. ğŸ—ï¸ Code Quality & Design (0â€“20 pts)
- Modularity and separation of concerns
- DRY (Don't Repeat Yourself) principle adherence
- SOLID principles (for OOP submissions)
- Detection of anti-patterns (God objects, deep nesting, spaghetti code)
- Appropriate use of data structures
- Error handling and input validation design

### 5. ğŸ§© Edge Case Handling (0â€“10 pts)
- Empty inputs / null values
- Boundary values (min/max, zero, negative numbers)
- Large inputs (performance under stress)
- Unexpected types or malformed data
- AI-generated edge case probing (beyond instructor-defined tests)

### 6. ğŸ’¬ Code Clarity & Communication (0â€“10 pts)
- Logical flow and reasoning clarity
- Documentation completeness
- Consistent coding style
- Self-explanatory structure without over-commenting

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FINAL SCORE BREAKDOWN              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Correctness              â”‚   /25           â”‚
â”‚ Efficiency               â”‚   /20           â”‚
â”‚ Code Quality & Design    â”‚   /20           â”‚
â”‚ Readability              â”‚   /15           â”‚
â”‚ Edge Case Handling       â”‚   /10           â”‚
â”‚ Clarity & Communication  â”‚   /10           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL                    â”‚   /100          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> ğŸ’¡ **Instructors can customize rubric weights** per assignment â€” a systems design problem might weight efficiency higher, while a beginner exercise might weight correctness and readability most.

---

## ğŸ”„ How It Works

### Step 1 â€” Submission Ingestion
Student submits code via the web UI, API, or LMS integration. The system auto-detects language (Python, JavaScript, Java, C++, Go, and more) and queues it for evaluation.

### Step 2 â€” Static Analysis
An AST (Abstract Syntax Tree) parser runs language-specific linting and complexity analysis without executing the code. Tools like ESLint, Pylint, Checkstyle, and Radon are used depending on the language.

### Step 3 â€” Sandboxed Execution
The code runs inside an isolated Docker container or WebAssembly sandbox with strict CPU, memory, and time limits. All instructor-defined test cases run, and additional AI-generated edge cases are automatically probed.

### Step 4 â€” AI Deep Analysis
The cleaned and anonymized code is sent to the AI evaluation layer. The LLM analyzes it against the rubric, identifies patterns, compares it to reference solutions (if provided), and generates nuanced observations about quality and design.

### Step 5 â€” Feedback Report Generation
A structured, detailed feedback report is assembled combining static analysis results, test outcomes, and AI observations into clear, actionable language tailored to the learner's level.

### Step 6 â€” Delivery & Storage
The report is delivered instantly to the student portal, logged in the instructor dashboard, and stored for longitudinal progress tracking.

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|---|---|
| **Backend API** | Python (FastAPI) |
| **AI Evaluation** | Anthropic Claude API / OpenAI GPT-4 |
| **Static Analysis** | AST parsers, Pylint, ESLint, Radon, Checkstyle |
| **Code Execution** | Docker (sandboxed containers), Judge0 |
| **Database** | PostgreSQL (submissions, scores), Redis (queue/cache) |
| **File Storage** | AWS S3 / MinIO |
| **Frontend** | React + TypeScript |
| **Auth** | OAuth2 / JWT (with SSO support for LMS) |
| **Queue** | Celery + RabbitMQ |
| **Containerization** | Docker + Kubernetes |
| **Monitoring** | Prometheus + Grafana |

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10+
- Docker & Docker Compose
- Node.js 18+
- An Anthropic or OpenAI API key

### 1. Clone the Repository

```bash
git clone https://github.com/your-org/ai-code-evaluator.git
cd ai-code-evaluator
```

### 2. Set Up Environment Variables

```bash
cp .env.example .env
```

Edit `.env` and fill in your credentials:

```env
# AI Provider
ANTHROPIC_API_KEY=your_api_key_here
AI_MODEL=claude-sonnet-4-5-20250929

# Database
DATABASE_URL=postgresql://user:password@localhost:5432/evaluator_db
REDIS_URL=redis://localhost:6379

# Execution Sandbox
SANDBOX_TYPE=docker          # options: docker, wasm, judge0
JUDGE0_API_URL=              # only if using Judge0
MAX_EXECUTION_TIME_MS=10000
MAX_MEMORY_MB=256

# Storage
S3_BUCKET=code-submissions
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=

# Security
SECRET_KEY=your_jwt_secret
ALLOWED_ORIGINS=http://localhost:3000
```

### 3. Launch with Docker Compose

```bash
docker-compose up --build
```

This starts:
- The FastAPI backend (`localhost:8000`)
- PostgreSQL database
- Redis instance
- Celery workers for async evaluation
- React frontend (`localhost:3000`)

### 4. Run Database Migrations

```bash
docker-compose exec api alembic upgrade head
```

### 5. Create Your First Assignment

```bash
curl -X POST http://localhost:8000/assignments \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Two Sum Problem",
    "description": "Given an array of integers, return indices of two numbers that add to target.",
    "language": "python",
    "rubric": {
      "correctness_weight": 30,
      "efficiency_weight": 25,
      "quality_weight": 20,
      "readability_weight": 15,
      "edge_cases_weight": 10
    },
    "test_cases": [
      {"input": "[2, 7, 11, 15], 9", "expected_output": "[0, 1]"},
      {"input": "[3, 2, 4], 6", "expected_output": "[1, 2]"}
    ]
  }'
```

### 6. Submit Code for Evaluation

```bash
curl -X POST http://localhost:8000/submissions \
  -H "Authorization: Bearer STUDENT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "assignment_id": "asgn_123",
    "language": "python",
    "code": "def two_sum(nums, target):\n    seen = {}\n    for i, num in enumerate(nums):\n        complement = target - num\n        if complement in seen:\n            return [seen[complement], i]\n        seen[num] = i"
  }'
```

---

## ğŸ“¡ API Reference

### Authentication

All endpoints require a Bearer token obtained via `/auth/login`.

### Core Endpoints

| Method | Endpoint | Description |
|---|---|---|
| `POST` | `/auth/login` | Authenticate and get access token |
| `POST` | `/assignments` | Create a new assignment (instructor) |
| `GET` | `/assignments/{id}` | Get assignment details |
| `POST` | `/submissions` | Submit code for evaluation |
| `GET` | `/submissions/{id}` | Get submission + full feedback report |
| `GET` | `/submissions/{id}/status` | Poll evaluation status |
| `GET` | `/assignments/{id}/analytics` | Class-wide submission analytics |
| `GET` | `/students/{id}/progress` | Individual student progress over time |
| `POST` | `/assignments/{id}/rubric` | Update rubric weights |

### Example Response â€” Submission Feedback

```json
{
  "submission_id": "sub_abc123",
  "status": "completed",
  "evaluated_at": "2025-02-14T10:30:00Z",
  "scores": {
    "correctness": 24,
    "efficiency": 18,
    "code_quality": 16,
    "readability": 13,
    "edge_cases": 8,
    "clarity": 9,
    "total": 88
  },
  "grade": "B+",
  "test_results": {
    "passed": 9,
    "failed": 1,
    "total": 10,
    "failed_cases": [
      {
        "input": "[], 0",
        "expected": "[]",
        "actual": "IndexError: list index out of range"
      }
    ]
  },
  "feedback": {
    "summary": "Strong solution with an efficient hash map approach. Minor gaps in edge case handling and one naming convention issue.",
    "strengths": [
      "Excellent use of a hash map â€” O(n) time complexity is optimal for this problem.",
      "Clean, Pythonic code structure with good use of enumerate().",
      "Handles the core algorithm correctly across 9 out of 10 test cases."
    ],
    "improvements": [
      {
        "category": "Edge Cases",
        "severity": "medium",
        "description": "The function crashes on empty input lists. Add a guard clause at the top.",
        "suggestion": "if not nums: return []",
        "line_reference": 1
      },
      {
        "category": "Readability",
        "severity": "low",
        "description": "The variable name 'seen' is slightly ambiguous. Consider 'num_to_index' for clarity.",
        "suggestion": "num_to_index = {}  # Maps each number to its index",
        "line_reference": 2
      }
    ],
    "complexity_analysis": {
      "time": "O(n)",
      "space": "O(n)",
      "assessment": "Optimal. Well done for avoiding the O(nÂ²) brute force approach."
    }
  }
}
```

---

## ğŸ“„ Feedback Report Format

Every student receives a report containing:

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  CODE EVALUATION REPORT
  Assignment: Two Sum Problem
  Submitted: Feb 14, 2025 at 10:30 AM
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  OVERALL SCORE: 88 / 100   Grade: B+

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Dimension          â”‚Score â”‚ Max      â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Correctness        â”‚  24  â”‚   25     â”‚
  â”‚ Efficiency         â”‚  18  â”‚   20     â”‚
  â”‚ Code Quality       â”‚  16  â”‚   20     â”‚
  â”‚ Readability        â”‚  13  â”‚   15     â”‚
  â”‚ Edge Cases         â”‚   8  â”‚   10     â”‚
  â”‚ Clarity            â”‚   9  â”‚   10     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  TEST RESULTS: 9/10 passed

  âœ… WHAT YOU DID WELL
  ...

  ğŸ“ˆ HOW TO IMPROVE
  ...

  ğŸ” COMPLEXITY ANALYSIS
  Time: O(n) â€” Optimal
  Space: O(n)

  ğŸ’¡ NEXT STEPS
  Try solving with the constraint of O(1) space.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“ Use Cases

**University Courses**
Professors assign weekly coding problems and automatically receive graded submissions with AI-generated insights about common class-wide misconceptions.

**Coding Bootcamps**
Instructors track learner progress over time, identify students who need help, and deliver consistent feedback regardless of cohort size.

**Technical Recruitment**
Hiring teams evaluate hundreds of take-home coding challenges with consistent rubrics â€” removing evaluator fatigue and implicit bias from the process.

**Self-Directed Learning**
Learners submit personal practice problems and receive mentor-quality feedback at any time, without waiting for a human reviewer.

**Internal Developer Upskilling**
Engineering teams run internal coding challenges and assessments with fair, automated scoring to support career development programs.

---

## ğŸ—ºï¸ Roadmap

### Phase 1 â€” MVP (Current)
- [x] Multi-language submission support (Python, JavaScript, Java, C++)
- [x] Sandboxed code execution
- [x] AI-powered feedback generation
- [x] Instructor dashboard
- [x] REST API

### Phase 2 â€” Q2 2025
- [ ] Plagiarism and AI-written code detection
- [ ] GitHub / GitLab repository submission support
- [ ] Canvas and Moodle LMS plugins
- [ ] Customizable rubric templates
- [ ] Batch submission processing (ZIP upload)

### Phase 3 â€” Q3 2025
- [ ] Student learning path recommendations based on weak areas
- [ ] Multi-language support expansion (Go, Rust, Kotlin, Swift)
- [ ] Pair programming replay and annotation
- [ ] Real-time collaborative evaluation for panel reviews

### Phase 4 â€” Q4 2025
- [ ] Adaptive test case generation using AI
- [ ] Long-form project evaluation (multi-file submissions)
- [ ] Longitudinal skills analytics and competency tracking
- [ ] Enterprise SSO and audit trail compliance

---

## ğŸ¤ Contributing

We welcome contributions! Please read [CONTRIBUTING.md](./CONTRIBUTING.md) first.

```bash
# Fork the repo, then:
git checkout -b feature/your-feature-name
git commit -m "feat: describe your change"
git push origin feature/your-feature-name
# Open a Pull Request
```

**Areas where we especially welcome help:**
- Adding support for new programming languages
- Improving rubric prompt engineering
- Writing test cases for the evaluator itself
- Accessibility improvements in the frontend
- Documentation and tutorials

---

## ğŸ“œ License

This project is licensed under the **MIT License** â€” see [LICENSE](./LICENSE) for details.

---

## ğŸ™ Acknowledgements

- [Anthropic](https://anthropic.com) for the Claude API powering AI evaluation
- [Judge0](https://judge0.com) for the open-source code execution sandbox
- The open-source community behind AST parsing and static analysis tooling

---

<div align="center">

**Built to make learning faster, fairer, and more human â€” at any scale.**

[â­ Star this repo](https://github.com/your-org/ai-code-evaluator) Â· [ğŸ› Report a Bug](https://github.com/your-org/ai-code-evaluator/issues) Â· [ğŸ’¡ Request a Feature](https://github.com/your-org/ai-code-evaluator/issues)

</div>
